Experiments, Results and Observations
In artificial neural networks, set of inputs is multiplied with a weight matrix, after which weighted sum is taken, then a bias is added to it, and finally the result is taken as input for an activation function. This model can be used to emulate any function just by knowing some of its values without actually knowing the function.
This property of neural networks is very handy and can be applied in cryptography to  do cryptanalysis. Here, the same has been applied to test the cryptanalysis strength of neural networks by applying neural networks to emulate block cipher just by entering a few  sets of plaintext and respective cipher text.
Cryptanalysis in Shift Cipher
The plain text that was used to train the network:
[18 8 ;11 4 ;13 2 ;4 8 ;18 6 ;14 11 ;3 4 ;13 19 ;7 4 ;12 0 ;13 8 ;18 22 ;0 11 ;10 8 ;13 6 ;8 13 ;19 7 ;4 17 ;0 8 ;13 19 ;7 4 ;3 14 ;6 18 ;0 17 ;4 1;0 17;10 8;13 6;15 7;4 1;0 10;4 17;24 22;0 18;2 11;14 18;4 3;19 8;11 11;13 8;13 4;19 14;15 17;14 19;4 18;19 0;6 0;8 13;18 19;19 7;4 13;4 22;19 0;23 11;0 22;18 19;7 8;18 4;13 17;0 6;4 3;19 7;4 2;20 18;19 14;12 4;17 18]
The above plain text is encypted with key = (0,2) and the result is as follows:
[20 10;13 6;15 4;6 10;20 8;16 13;5 6;15 21;9 6;14 2;15 10;20 24;2 13;12 10;15 8;10 15;21 9;6 19;2 10;15 21;9 6;5 16;8 20;2 19;6 3;2 19;12 10;15 8;17 9;6 3;2 12;6 19;0 24;2 20;4 13;16 20;6 5;21 10;13 13;15 10;15 6;21 16;17 19;16 21;6 20;21 2;8 2;10 15;20 21;21 9;6 15;6 24;21 2;25 13;2 24;20 21;9 10;20 6;15 19;2 8;6 5;21 9;6 4;22 20;21 16;14 6;19 20]
The cipher text for the above text was normalised with values ranging from 0 to 1. Normalisation is done in blocks of 2.  For example if the cipher text is 20 10, then its normalized value will be (20*26+10)/675=0.7851852. The normalised cipher is as follows:
[0.785185;0.50963;0.583704;0.245926;0.782222;0.635556;0.201481;0.608889;0.355556;0.542222;0.592593;0.805926;0.0962963;0.477037;0.58963;0.407407;0.822222;0.259259;0.0918519;0.608889;0.355556;0.216296;0.337778;0.105185;0.235556;0.105185;0.477037;0.58963;0.668148;0.235556;0.0948148;0.259259;0.0355556;0.106667;0.173333;0.645926;0.238519;0.823704;0.52;0.592593;0.586667;0.832593;0.682963;0.647407;0.260741;0.811852;0.311111;0.407407;0.801481;0.822222;0.253333;0.266667;0.811852;0.982222;0.112593;0.801481;0.361481;0.779259;0.605926;0.0888889;0.238519;0.822222;0.237037;0.877037;0.832593;0.548148;0.761481]
4.1. Training with 3 neurons in the hidden layer
 At first, around 25 test cases were taken to train the neural network with a hidden layer containing 3 neurons. The results got close to the actual values, but, still considerable error was present. The variance obtained was 0.00344057.
This was desired  result: [0.785185;0.50963;0.583704;0.245926;0.782222;0.635556;0.201481;0.608889;0.355556;0.542222;0.592593;0.805926;0.0962963;0.477037;0.58963;0.407407;0.822222;0.259259;0.0918519;0.608889;0.355556;0.216296;0.337778;0.105185;0.235556]
Following was obtained result after training of networks:
 [0.68892131] [0.45284563] [0.52006249] [0.17000021] [0.68357053] [0.63842567] [0.20413324] [0.5505784 ] [0.30390393] [0.44935422] [0.58541445] [0.68554398] [0.11546045] [0.45206001] [0.56545289] [0.29811431] [0.69481469] [0.15853208] [0.11567932] [0.5505784 ] [0.30390393] [0.1361429 ] [0.21028752] [0.12062347] [0.24967586]
As we can clearly see, there are a lot of errors
4.2. Training with Lavenberg-Marquardt method and 10 neurons in the hidden layer
Then, another neural network was created, this time with hidden layer containing 10 neurons, and applying Lavenberg-Marquardt learning method, with learning being done with the help of about 67 test cases. Training was done with 70% of test cases and result was tested and validated with the rest 30%. 
Following was the desired  results matrix: [0.785185;0.50963;0.583704;0.245926;0.782222;0.635556;0.201481;0.608889;0.355556;0.542222;0.592593;0.805926;0.0962963;0.477037;0.58963;0.407407;0.822222;0.259259;0.0918519;0.608889;0.355556;0.216296;0.337778;0.105185;0.235556;0.105185;0.477037;0.58963;0.668148;0.235556;0.0948148;0.259259;0.0355556;0.106667;0.173333;0.645926;0.238519;0.823704;0.52;0.592593;0.586667;0.832593;0.682963;0.647407;0.260741;0.811852;0.311111;0.407407;0.801481;0.822222;0.253333;0.266667;0.811852;0.982222;0.112593;0.801481;0.361481;0.779259;0.605926;0.0888889;0.238519;0.822222;0.237037;0.877037;0.832593;0.548148;0.761481]

And following was the results obtained through training:
output =
 0.8001    0.4373    0.4695    0.2248    0.7735    0.6210    0.2297    0.5411    0.3078
    0.3494    0.5792    0.6463    0.1169    0.4613    0.5606    0.4625    0.7950    0.3483
    0.1015    0.5411    0.3078    0.2794    0.4569    0.2406    0.3150    0.2406    0.4613
    0.5606    0.6601    0.3150    0.0979    0.3483    0.6963    0.2475    0.2042    0.5785
    0.2582    0.8086    0.5368    0.5792    0.5270    0.8338    0.6338    0.5623    0.3668
    0.8495    0.2996    0.4625    0.7033    0.7950    0.2941    0.4490    0.8495    0.7833
    0.2800    0.7033    0.3203    0.7789    0.5595    0.1831    0.2582    0.7950    0.2871
    0.7348    0.8338    0.4826    0.7081
Removing normalization, following ciphertext was obtained:
20 20;11 9;12 5;5 22;20 2;16 3;5 25;14 1;8 0;9 2;15 1;16 20;3 1;11 25;14 14;12 0;20 17;9 1;2 17;14 1;8 0;7 7;11 22;6 6;8 5;6 6;11 25;14 14;17 4;8 5;2 14;9 1;18 2;6 11;5 8;15 0;6 18;21 0;13 24;15 1;13 18;21 17;16 12;14 16;9 14;22 1;7 20;12 0;18 7;20 17;7 17;11 17;22 1;20 9;7 7;18 7;8 8;20 6;14 14;4 20;6 18;20 17;7 12;19 2;21 17;12 14;18 10;
66 errors
variance = 0.0126302

4.3. Training with Lavenberg-Marquardt method and 15 neurons in the hidden layer
 Next, training was done with 15 neurons in the hidden layer, with the same training method which yielded  the output as: 
 0.7893    0.4991    0.5601    0.2323    0.7819    0.6360    0.1907    0.5965    0.3643    0.4567    0.5965    0.8047
    0.1018    0.4588    0.5845    0.3995    0.8144    0.2334    0.0887    0.5965    0.3643    0.1982    0.3017    0.1098
    0.2281    0.1098    0.4588    0.5845    0.6884    0.2281    0.1121    0.2334    0.8395    0.1124    0.1635    0.6280
    0.2299    0.8214    0.4818    0.5965    0.5759    0.8296    0.6741    0.6258    0.2315    0.7824    0.2955    0.3995
    0.7977    0.8144    0.2583    0.2494    0.7824    0.9759    0.1222    0.7977    0.3665    0.7682    0.6047    0.0943
    0.2299    0.8144    0.2274    0.8669    0.8296    0.5353    0.7563

Removing normalization, following ciphertext was obtained:
20 13;12 25;14 14;6 1;20 8;16 13;4 25;15 13;9 12;11 22;15 13;20 23;2 17;11 24;15 5;10 10;21 4;6 2;2 8;15 13;9 12;5 4;7 22;2 22;5 24;2 22;11 24;15 5;17 23;5 24;2 24;6 2;21 21;2 24;4 6;16 8;5 25;21 8;12 13;15 13;14 25;21 14;17 13;16 6;6 0;20 8;7 17;10 10;20 18;21 4;6 18;6 12;20 8;25 9;3 4;20 18;9 13;19 25;15 18;2 12;5 25;21 4;5 23;22 13;21 14;13 23;19 17;
65 errors
variance = 0.00994988
4.4.  Training with Lavenberg-Marquardt method and 20 neurons in the hidden layer
 Next, training was done with 20 neurons in the hidden layer, with the same training method which yielded  the output as:
    0.7893    0.4991    0.5601    0.2323    0.7819    0.6360    0.1907    0.5965    0.3643    0.4567    0.5965    0.8047
    0.1018    0.4588    0.5845    0.3995    0.8144    0.2334    0.0887    0.5965    0.3643    0.1982    0.3017    0.1098
    0.2281    0.1098    0.4588    0.5845    0.6884    0.2281    0.1121    0.2334    0.8395    0.1124    0.1635    0.6280
    0.2299    0.8214    0.4818    0.5965    0.5759    0.8296    0.6741    0.6258    0.2315    0.7824    0.2955    0.3995
    0.7977    0.8144    0.2583    0.2494    0.7824    0.9759    0.1222    0.7977    0.3665    0.7682    0.6047    0.0943
    0.2299    0.8144    0.2274    0.8669    0.8296    0.5353    0.7563 

Removing normalization, following ciphertext was obtained:
20 13;12 25;14 14;6 1;20 8;16 13;4 25;15 13;9 12;11 22;15 13;20 23;2 17;11 24;15 5;10 10;21 4;6 2;2 8;15 13;9 12;5 4;7 22;2 22;5 24;2 22;11 24;15 5;17 23;5 24;2 24;6 2;21 21;2 24;4 6;16 8;5 25;21 8;12 13;15 13;14 25;21 14;17 13;16 6;6 0;20 8;7 17;10 10;20 18;21 4;6 18;6 12;20 8;25 9;3 4;20 18;9 13;19 25;15 18;2 12;5 25;21 4;5 23;22 13;21 14;13 23;19 17;
65 errors
variance = 0.00994988
4.5.  Training with Bayessian regularisation method and 10 neurons in the hidden layer
Next, training was done with 10 neurons in hidden layer, applying the Bayessian regularisation method, which yielded  the output as:
 0.7852    0.5096    0.5837    0.2459    0.7822    0.6356    0.2015    0.6089    0.3556    0.5422    0.5926    0.8059
    0.0963    0.4770    0.5896    0.4074    0.8222    0.2593    0.0919    0.6089    0.3556    0.2163    0.3378    0.1052
    0.2356    0.1052    0.4770    0.5896    0.6681    0.2356    0.0948    0.2593    0.0356    0.1067    0.1733    0.6459
    0.2385    0.8237    0.5200    0.5926    0.5867    0.8326    0.6830    0.6474    0.2607    0.8119    0.3111    0.4074
    0.8015    0.8222    0.2533    0.2667    0.8119    0.9822    0.1126    0.8015    0.3615    0.7793    0.6059    0.0889
    0.2385    0.8222    0.2370    0.8772    0.8326    0.5481    0.7615

Removing normalization, following ciphertext was obtained:
20 10;13 6;15 4;6 10;20 8;16 13;5 6;15 21;9 6;14 2;15 10;20 24;2 13;12 10;15 8;10 15;21 9;6 19;2 10;15 21;9 6;5 16;8 20;2 19;6 3;2 19;12 10;15 8;17 9;6 3;2 12;6 19;0 24;2 20;4 13;16 20;6 5;21 10;13 13;15 10;15 6;21 16;17 19;16 21;6 20;21 2;8 2;10 15;20 21;21 9;6 15;6 24;21 2;25 13;2 24;20 21;9 10;20 6;15 19;2 8;6 5;21 9;6 4;22 20;21 16;14 6;19 20;
1 errors
variance = 1.20221e-09

4.6.  Training with Bayessian regularisation method and 15 neurons in the hidden layer
 
 Next, training was done with 15 neurons in hidden layer, applying the Bayessian regularisation method, which yielded   the output as:
 0.7852    0.5096    0.5837    0.2458    0.7823    0.6356    0.2015    0.6089    0.3555    0.5422    0.5926    0.8059
    0.0963    0.4770    0.5897    0.4074    0.8222    0.2592    0.0919    0.6089    0.3555    0.2162    0.3377    0.1052
    0.2356    0.1052    0.4770    0.5897    0.6682    0.2356    0.0949    0.2592    1.0365    0.1067    0.1733    0.6460
    0.2385    0.8237    0.5200    0.5926    0.5867    0.8326    0.6830    0.6475    0.2607    0.8118    0.3111    0.4074
    0.8015    0.8222    0.2532    0.2666    0.8118    0.9820    0.1126    0.8015    0.3614    0.7793    0.6060    0.0890
    0.2385    0.8222    0.2370    0.8770    0.8326    0.5482    0.7615

Removing normalization, following ciphertext was obtained:
20 10;13 6;15 4;6 10;20 8;16 13;5 6;15 21;9 6;14 2;15 10;20 24;2 13;12 10;15 8;10 15;21 9;6 19;2 10;15 21;9 6;5 16;8 20;2 19;6 3;2 19;12 10;15 8;17 9;6 3;2 12;6 19;26 24;2 20;4 13;16 20;6 5;21 10;13 13;15 10;15 6;21 16;17 19;16 21;6 20;21 2;8 2;10 15;20 21;21 9;6 15;6 24;21 2;25 13;2 24;20 21;9 10;20 6;15 19;2 8;6 5;21 9;6 4;22 20;21 16;14 6;19 20;
2 errors
variance = 0.0149536
4.7. Training with Bayessian regularisation method and 20 neurons in the hidden layer

Next, training was done with 20 neurons in hidden layer, applying the Bayessian regularisation method, which yielded  the output as:
 0.7852    0.5096    0.5837    0.2459    0.7822    0.6356    0.2015    0.6089    0.3556    0.5422    0.5926    0.8059
    0.0963    0.4770    0.5896    0.4074    0.8222    0.2593    0.0919    0.6089    0.3556    0.2163    0.3378    0.1052
    0.2356    0.1052    0.4770    0.5896    0.6681    0.2356    0.0948    0.2593    0.0356    0.1067    0.1733    0.6459
    0.2385    0.8237    0.5200    0.5926    0.5867    0.8326    0.6830    0.6474    0.2607    0.8119    0.3111    0.4074
    0.8015    0.8222    0.2533    0.2667    0.8119    0.9822    0.1126    0.8015    0.3615    0.7793    0.6059    0.0889
    0.2385    0.8222    0.2370    0.8770    0.8326    0.5481    0.7615

Removing normalization, following ciphertext was obtained:
20 10;13 6;15 4;6 10;20 8;16 13;5 6;15 21;9 6;14 2;15 10;20 24;2 13;12 10;15 8;10 15;21 9;6 19;2 10;15 21;9 6;5 16;8 20;2 19;6 3;2 19;12 10;15 8;17 9;6 3;2 12;6 19;0 24;2 20;4 13;16 20;6 5;21 10;13 13;15 10;15 6;21 16;17 19;16 21;6 20;21 2;8 2;10 15;20 21;21 9;6 15;6 24;21 2;25 13;2 24;20 21;9 10;20 6;15 19;2 8;6 5;21 9;6 4;22 20;21 16;14 6;19 20;
1 errors
variance = 8.26088e-10

4.8. Training with  Solved conjugate Gradient method method and 10 neurons in the hidden layer

Next, training was done with 10 neurons in hidden layer, applying the Solved conjugate Gradient method, which yielded  the output as
    output =
 0.7373    0.5811    0.6069    0.2392    0.7206    0.6747    0.1758    0.5708    0.3821    0.6202    0.6118    0.7065
    0.0974    0.4936    0.6079    0.3921    0.7866    0.2511    0.0973    0.5708    0.3821    0.1770    0.4300    0.1004
    0.2427    0.1004    0.4936    0.6079    0.6382    0.2427    0.0977    0.2511    0.7478    0.1054    0.1396    0.6316
    0.2236    0.7873    0.5185    0.6118    0.6046    0.7431    0.6832    0.6148    0.2645    0.8527    0.4211    0.3921
    0.7184    0.7866    0.2392    0.3013    0.8527    0.9948    0.1704    0.7184    0.3356    0.7343    0.6045    0.0949
    0.2236    0.7866    0.2291    0.7326    0.7431    0.5958    0.7126

Removing normalization, following ciphertext was obtained:
19 4;15 2;15 20;6 5;18 18;17 13;4 15;14 21;9 24;16 3;15 23;18 9;2 14;12 21;15 20;10 5;20 11;6 13;2 14;14 21;9 24;4 15;11 4;2 16;6 8;2 16;12 21;15 20;16 15;6 8;2 14;6 13;19 11;2 19;3 16;16 10;5 21;20 11;13 12;15 23;15 18;19 8;17 19;15 25;6 23;22 4;10 24;10 5;18 17;20 11;6 5;7 21;22 4;25 21;4 11;18 17;8 19;19 2;15 18;2 12;5 21;20 11;5 25;19 1;19 8;15 12;18 13;
66 errors
variance = 0.00958247
4.9. Training with  Solved conjugate Gradient method method and 15 neurons in the hidden layer

Next, training was done with 15 neurons in hidden layer, applying the Solved conjugate Gradient method, which yielded  the output as
    output =
 0.7725    0.5736    0.6606    0.3082    0.7992    0.6591    0.1872    0.4599    0.3389    0.4780    0.5099    0.7564
    0.0753    0.4608    0.5377    0.4350    0.8356    0.2278    0.1118    0.4599    0.3389    0.1757    0.2375    0.1721
    0.2580    0.1721    0.4608    0.5377    0.6154    0.2580    0.0908    0.2278    0.9289    0.1986    0.1047    0.5697
    0.2400    0.8208    0.6648    0.5099    0.6672    0.7729    0.6888    0.5639    0.2312    0.7022    0.3067    0.4350
    0.9128    0.8356    0.1958    0.2420    0.7022    0.9583    0.2329    0.9128    0.4474    0.7649    0.4963    0.0593
    0.2400    0.8356    0.2494    0.9586    0.7729    0.6271    0.8689

Removing normalization, following ciphertext was obtained:
20 1;14 23;17 4;8 0;20 19;17 3;4 22;11 24;8 21;12 11;13 6;19 17;1 25;11 25;13 25;11 8;21 18;5 24;2 23;11 24;8 21;4 15;6 4;4 12;6 18;4 12;11 25;13 25;15 25;6 18;2 9;5 24;24 3;5 4;2 19;14 21;6 6;21 8;17 7;13 6;17 8;20 2;17 23;14 17;6 0;18 6;7 25;11 8;23 18;21 18;5 2;6 7;18 6;24 23;6 1;23 18;11 16;19 22;12 23;1 14;6 6;21 18;6 12;24 23;20 2;16 7;22 15;
67 errors
variance = 0.0162379
4.10. Training with  Solved conjugate Gradient method method and 20 neurons in the hidden layer

Next, training was done with 20 neurons in hidden layer, applying the Solved conjugate Gradient method, which yielded  the output as
    output =
     0.7568    0.5050    0.6254    0.2614    0.7806    0.6701    0.2164    0.6168    0.3344    0.5537    0.6058    0.6912
    0.1087    0.4576    0.5994    0.4179    0.8167    0.2538    0.0643    0.6168    0.3344    0.2069    0.3304    0.1523
    0.2280    0.1523    0.4576    0.5994    0.6575    0.2280    0.0828    0.2538    0.0327    0.1281    0.1584    0.6417
    0.2308    0.8129    0.5185    0.6058    0.6019    0.7179    0.6605    0.6673    0.2439    0.8104    0.2527    0.4179
    0.8060    0.8167    0.2585    0.2761    0.8104    0.9942    0.1267    0.8060    0.3870    0.8043    0.5976    0.1215
    0.2308    0.8167    0.2289    0.7890    0.7179    0.5516    0.7607

Removing normalization, following ciphertext was obtained:
19 17;13 3;16 6;6 20;20 7;17 10;5 16;16 0;8 18;14 10;15 19;17 25;2 21;11 23;15 15;10 22;21 5;6 15;1 17;16 0;8 18;5 10;8 15;3 25;5 24;3 25;11 23;15 15;17 2;5 24;2 4;6 15;0 22;3 8;4 3;16 17;6 0;21 3;13 12;15 19;15 16;18 17;17 4;17 8;6 9;21 1;6 15;10 22;20 24;21 5;6 18;7 4;21 1;25 21;3 8;20 24;10 1;20 23;15 13;3 4;6 0;21 5;5 25;20 13;18 17;14 8;19 19;
67 errors
variance = 0.00102834

4.11. Normalization  of RSA cipher: 
Two letters are treated as a single number and encryption is done, then, the result obtained is divided by n, to get value from 0 to 1. e.g. 
p=15485863              q=15535397
n=p*q
n = 240579029592611
d = 5     e = 96231599428541
Suppose if the following is plain text:
18 8 ;11 4 ;13 2 ;4 8 ;18 6 ;14 11 ;3 4 ;13 19 ;7 4 ;12 0 ;13 8 ;18 22 ;0 11 ;10 8 ;13 6 ;8 13 ;19 7 ;4 17 ;0 8 ;13 19 ;7 4 ;3 14 ;6 18 ;0 17 ;4 1;0 17;10 8;13 6;15 7;4 1;0 10;4 17;24 22;0 18;2 11;14 18;4 3;19 8;11 11;13 8;13 4;19 14;15 17;14 19;4 18;19 0;6 0;8 13;18 19;19 7;4 13;4 22;19 0;23 11;0 22;18 19;7 8;18 4;13 17;0 6;4 3;19 7;4 2;20 18;19 14;12 4;17 18
Then, it is converted into normalized cipher text by using the program shown in research methods and logic given above to obtain the following:
[0.303479;0.816911;0.552391;0.0469941;0.860304;0.247556;0.0107922;0.594578;0.718796;0.343046;0.914059;0.461098;6.69431e-10;0.325585;0.792763;0.476369;0.832441;0.052411;1.36205e-10;0.594578;0.718796;0.0126879;0.374701;5.90183e-09;0.0430987;5.90183e-09;0.325585;0.792763;0.307882;0.0430987;4.15664e-10;0.052411;0.427951;7.85425e-09;0.00173842;0.829964;0.0441842;0.107591;0.0357845;0.914059;0.672208;0.770653;0.39413;0.914109;0.0530425;0.922478;0.32322;0.476369;0.776246;0.832441;0.0499451;0.0556294;0.922478;0.994572;2.14218e-08;0.776246;0.73945;0.419088;0.469147;3.2322e-11;0.0441842;0.832441;0.0436388;0.106698;0.770653;0.516583;0.209689]
